import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.*;
import java.net.URI;
import java.text.NumberFormat;
import java.util.HashMap;
import java.util.Map;

/**
 * Created by: ccong
 * Date: 18/9/16 下午9:24
 *  This map is used to calculate the last result
 *
 */
public class MyMapper3 extends Mapper<LongWritable, Text,Text, Text> {

    Logger logger = LoggerFactory.getLogger(MyMapper3.class);

    public static Map<String,Integer> countMap = new HashMap<>();//"count" : 1213
    public static Map<String,Integer> dfMap = new HashMap<>();;//"Nike": 23

    protected void setup(Context context) throws IOException, InterruptedException {
        if(countMap == null || countMap.size()==0 || dfMap==null || dfMap.size()==0) {
            /*
               Get 'N' cache files "/usr/file/mr/pub/output1/part-r-00003"
               and 'DF' cache file : "/usr/file/mr/pub/output1/part-r-00000"
            */
            URI[] uris = context.getCacheFiles();
            //get the distributed file system and use it to read cached file
            FileSystem fs = FileSystem.get(context.getConfiguration());
            if(uris!=null) {
                for(URI uri : uris) {
                    if(uri.getPath().endsWith("part-r-00003")) {
                        //uri.getPath() : /usr/file/mr/pub/output1/part-r-00003
                        Path path = new Path(uri.getPath());
                        FSDataInputStream fsin = fs.open(path);
                        DataInputStream dis = new DataInputStream(fsin);
                        BufferedReader br = new BufferedReader(new InputStreamReader(dis));
                        String line = br.readLine();
                        //only one line
                        if(line.startsWith("count")) {
                            String[] arr = line.split("\t");
                            countMap.put(arr[0],Integer.parseInt(arr[1].trim().toString()));
                        }
                        br.close();
                        dis.close();
                        fsin.close();
                    } else if(uri.getPath().endsWith("part-r-00000")) {
                        Path path = new Path(uri.getPath());
                        FSDataInputStream fsin = fs.open(path);
                        DataInputStream dis = new DataInputStream(fsin);
                        BufferedReader br = new BufferedReader(new InputStreamReader(dis));
                        String line = null;
                        while((line = br.readLine())!=null){
                            String[] arr = line.split("\t");
                            dfMap.put(arr[0],Integer.parseInt(arr[1].trim().toString()));
                        }
                        br.close();
                        dis.close();
                        fsin.close();
                    }
                }
            }
        }
    }

    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        FileSplit fs = (FileSplit) context.getInputSplit();
        //Read all TF file generated by Map1 (we have set up the output1 as the input path)
        if(!fs.getPath().getName().contains("part-r-00003")) {
            String line = value.toString().trim();
            //split : word_id 3
            String[] values = line.split("\t");
            if(values.length>=2) {
                int tf = Integer.parseInt(values[1].trim());
                String wordAndId = values[0];
                String[] arr = wordAndId.split("_");
                String word = arr[0];
                String userId  = arr[1];

                //calculate the weight : W = TF*Log(N/DF)
                if(dfMap.get(word)!=null) {
                    double result = tf*Math.log(countMap.get("count")/dfMap.get(word));
                    NumberFormat nf = NumberFormat.getInstance();
                    nf.setMaximumFractionDigits(5);
                    context.write(new Text(word),new Text(userId+":"+nf.format(result)));
                }
            }

        }
    }
}
